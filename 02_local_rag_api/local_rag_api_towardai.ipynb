{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, StorageContext, VectorStoreIndex\n",
    "from llama_index.core.service_context import ServiceContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "import requests\n",
    "import os\n",
    "import qdrant_client\n",
    "from tqdm import tqdm\n",
    "import arxiv\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import yaml\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Data:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def create_data_folder(self, download_path):\n",
    "        if not os.path.exists(download_path):\n",
    "            os.makedirs(download_path)\n",
    "            print(\"Output folder created\")\n",
    "        else:\n",
    "            print(\"Output folder already exists.\")\n",
    "\n",
    "    def download_papers(self, search_query, download_path, server=\"arxiv\", start_date=None, end_date=None):\n",
    "        self.create_data_folder(download_path)\n",
    "        \n",
    "        if server == \"arxiv\":\n",
    "            client = arxiv.Client()\n",
    "\n",
    "            search = arxiv.Search(\n",
    "                query=search_query,\n",
    "                sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "            )\n",
    "\n",
    "            results = list(client.results(search))\n",
    "            for paper in tqdm(results):\n",
    "                if os.path.exists(download_path):\n",
    "                    paper_title = (paper.title).replace(\" \", \"_\")\n",
    "                    paper.download_pdf(dirpath=download_path, filename=f\"{paper_title}.pdf\")\n",
    "                    print(f\"{paper.title} Downloaded.\")\n",
    "\n",
    "        elif server == \"medrxiv\":\n",
    "            if not start_date or not end_date:\n",
    "                print(\"Error: 'start_date' and 'end_date' are required for medRxiv.\")\n",
    "                return\n",
    "\n",
    "            # Construct the API URL\n",
    "            api_url = f\"https://api.medrxiv.org/details/{server}/{start_date}/{end_date}/0/json\"\n",
    "\n",
    "            response = requests.get(api_url)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to retrieve data from MedRxiv API. Status code: {response.status_code}\")\n",
    "                return\n",
    "\n",
    "            data = response.json()\n",
    "\n",
    "            if 'collection' not in data or len(data['collection']) == 0:\n",
    "                print(\"No papers found with the given search query.\")\n",
    "                return\n",
    "\n",
    "            papers = data['collection']\n",
    "\n",
    "            for paper in tqdm(papers):\n",
    "                title = paper['title'].strip().replace(\" \", \"_\").replace(\"/\", \"_\")  # Replace spaces and slashes with underscores\n",
    "                pdf_url = f\"https://www.medrxiv.org/content/{paper['doi']}.full.pdf\"\n",
    "                print(f\"Attempting to download {title} from {pdf_url}\")\n",
    "\n",
    "                try:\n",
    "                    pdf_response = requests.get(pdf_url)\n",
    "                    if pdf_response.status_code == 200:\n",
    "                        pdf_path = os.path.join(download_path, f\"{title}.pdf\")\n",
    "                        with open(pdf_path, 'wb') as pdf_file:\n",
    "                            pdf_file.write(pdf_response.content)\n",
    "                        print(f\"{title} Downloaded to {pdf_path}.\")\n",
    "                    else:\n",
    "                        print(f\"Failed to download {title}. Status code: {pdf_response.status_code}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred while downloading {title}: {e}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Server '{server}' is not supported.\")\n",
    "    \n",
    "    def ingest(self, embedder, llm):\n",
    "        print(\"Reading pdf from the specified directory...\")\n",
    "        documents = SimpleDirectoryReader(self.config[\"data_path\"]).load_data()\n",
    "\n",
    "        print(\"Initializing Qdrant client...\")\n",
    "        client = qdrant_client.QdrantClient(url=self.config[\"qdrant_url\"])\n",
    "        vector_store = QdrantVectorStore(\n",
    "            client=client,\n",
    "            collection_name=self.config[\"collection_name\"]\n",
    "        )\n",
    "        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "        # Use the Settings object correctly\n",
    "        service_context = ServiceContext.from_defaults(\n",
    "            llm=llm, embed_model=embedder, chunk_size=self.config[\"chunk_size\"]\n",
    "        )\n",
    "        print(\"Start embedding the documents and storing the resulting vectors in the Qdrant vector store...\")\n",
    "        index = VectorStoreIndex.from_documents(\n",
    "            documents, storage_context=storage_context, service_context=service_context\n",
    "        )\n",
    "        print(\n",
    "            f\"Data indexed successfully to Qdrant. Collection: {self.config['collection_name']}\"\n",
    "        )\n",
    "        return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually set the parameters\n",
    "query = \"heart failure exercise tolerance\"  # The topic you are interested in\n",
    "ingest = True  # Set to True if you want to ingest data\n",
    "server = \"medrxiv\"  # Set server to \"medrxiv\" or \"arxiv\"\n",
    "start_date = \"2003-07-31\"  # Required for medRxiv, it is useful becacuse you don't want outdated papers\n",
    "end_date = \"2024-08-01\"  # Required for medRxiv, it is useful becacuse you don't want outdated papers\n",
    "\n",
    "# Set the configuration file path\n",
    "config_file = \"config.yml\"\n",
    "\n",
    "# Load the configuration\n",
    "with open(config_file, \"r\") as conf:\n",
    "    config = yaml.safe_load(conf)\n",
    "\n",
    "# Initialize the Data object with the configuration\n",
    "data = Data(config)\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_fixed(10))\n",
    "def download_papers_with_retry(data, search_query, download_path, server=\"arxiv\", start_date=None, end_date=None):\n",
    "    data.download_papers(search_query=search_query, download_path=download_path, server=server, start_date=start_date, end_date=end_date)\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_fixed(10))\n",
    "def ingest_data_with_retry(data, embed_model, llm):\n",
    "    data.ingest(embedder=embed_model, llm=llm)\n",
    "\n",
    "# If a query is provided, download papers\n",
    "if query:\n",
    "    download_papers_with_retry(data, query, config[\"data_path\"], server=server, start_date=start_date, end_date=end_date)\n",
    "\n",
    "# If ingest flag is set, ingest data to Qdrant vector database\n",
    "if ingest:\n",
    "    print(\"Loading Embedder...\")\n",
    "    embed_model = LangchainEmbedding(\n",
    "        HuggingFaceEmbeddings(model_name=config[\"embedding_model\"])\n",
    "    )\n",
    "    llm = Ollama(model=config[\"llm_name\"], base_url=config[\"llm_url\"])\n",
    "    ingest_data_with_retry(data, embed_model, llm)\n",
    "    \n",
    "# 13 mins to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Querying\n",
    "\n",
    "Now that we’ve successfully loaded our data (research papers) into our vector store (Qdrant), we can begin querying it to retrieve relevant data for feeding to our LLM.\n",
    "\n",
    "Let’s begin by crafting a function that sets up our Qdrant index, which will serve as our query engine.\n",
    "\n",
    "Query Engine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rag.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/ollama/ollama/blob/main/docs/modelfile.md#build-from-llama3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enter the command in Terminal:ollama show --modelfile llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading the embedder\n",
    "\n",
    "\n",
    "\n",
    "class RAG:\n",
    "    def __init__(self, config_file, llm):\n",
    "        self.config = config_file\n",
    "        self.qdrant_client = qdrant_client.QdrantClient(\n",
    "            url=self.config['qdrant_url']\n",
    "        )\n",
    "        self.llm = llm  # ollama llm\n",
    "    \n",
    "    def load_embedder(self):\n",
    "        embed_model = LangchainEmbedding(\n",
    "            HuggingFaceEmbeddings(model_name=self.config['embedding_model'])\n",
    "        )\n",
    "        return embed_model\n",
    "\n",
    "\n",
    "    def qdrant_index(self):\n",
    "        client = qdrant_client.QdrantClient(url=self.config[\"qdrant_url\"])\n",
    "        qdrant_vector_store = QdrantVectorStore(\n",
    "            client=client, collection_name=self.config['collection_name']\n",
    "        )\n",
    "\n",
    "        service_context = ServiceContext.from_defaults(\n",
    "            llm=self.llm, embed_model=self.load_embedder(), chunk_size=self.config[\"chunk_size\"]\n",
    "        )\n",
    "\n",
    "        index = VectorStoreIndex.from_vector_store(\n",
    "            vector_store=qdrant_vector_store, service_context=service_context\n",
    "        )\n",
    "        return index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we create our model using the Modelfile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the Modelfile:\n",
    "\n",
    "1. Save it as a file (e.g. Modelfile)\n",
    "2. ollama create choose-a-model-name -f <<<location of the file e.g. ./Modelfile>>>'\n",
    "3. ollama run choose-a-model-name\n",
    "4. Start using the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ollama create ollama3_research -f Modelfile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows:\n",
    "NAME                    ID              SIZE    MODIFIED\n",
    "ollama3_research:latest 37e83b2fef40    4.7 GB  22 seconds ago\n",
    "\n",
    "4.7 GB, but actually it is not, it is just a reference to the original llama3 model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ollama run ollama3_research\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Ollama runs on http://localhost:11434\n",
    "\n",
    "Finally, we create an API endpoint using FastAPI. This endpoint will receive a query, search the documents, and return a response.\n",
    "\n",
    "An advantage of using FastAPI is its compatibility with Pydantic, which is very helpful in structuring our code and API responses.\n",
    "\n",
    "Let’s begin by defining two models: one for the Query and one for the Response:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install fastapi pydantic uvicorn pyyaml llama-index ollama qdrant-client\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.llamaindex.ai/en/v0.10.17/examples/callbacks/LlamaDebugHandler.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uvicorn app:app --host 0.0.0.0 --port 8000 --reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
